{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "_r4UUCGMTYac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1QTyYcpIJuw"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import re\n",
        "import streamlit as st\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "from PIL import Image\n",
        "import torch\n",
        "import ffmpeg\n",
        "import speech_recognition as sr\n",
        "from pytube import YouTube\n",
        "import string\n",
        "import whisper\n",
        "from moviepy.editor import AudioFileClip\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from key import openai_api_key\n",
        "from langchain import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "import monkey_patch     # OBS: esse arquivo evita um bug frequente do pytube\n",
        "from pytube import YouTube\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o que corta o texto em chunks:\n",
        "def get_chunks(texto_transcrito):\n",
        "    \"\"\"Fun√ß√£o que recebe uma string (um texto transcrito) e devolve\n",
        "    uma lista de strings, em que cada string (\"chunk\") cont√©m um\n",
        "    tamanho espec√≠fico, com overlap entre os peda√ßos. \"\"\"\n",
        "    text_splitter=CharacterTextSplitter(separator='.', chunk_size=1000, chunk_overlap=100, length_function=len)\n",
        "    chunks = text_splitter.split_text(texto_transcrito)\n",
        "    return chunks\n",
        "\n",
        "  # Criando a fun√ß√£o de vectorstore para transformar os chunks de texto em embeddings:\n",
        "def get_vectorstore(chunks):\n",
        "    \"\"\"Fun√ß√£o que recebe uma lista de chunks de texto e\n",
        "    os converte em embeddings (representa√ß√µes num√©ricas)\n",
        "    usando um modelo de embeddings pr√©-treinado. \"\"\"\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o para converter o v√≠deo para o formato adequado:\n",
        "@st.cache_data\n",
        "def convert_mp3_to_wav(mp3_file, wav_file):\n",
        "    \"\"\"Fun√ß√£o que converte um arquivo de v√≠deo no formato MP3 para um arquivo de\n",
        "    √°udio no formato WAV a partir das seguintes entradas: o caminho do arquivo\n",
        "    de v√≠deo no formato MP3 que se deseja converter; o caminho onde o arquivo\n",
        "    de √°udio WAV resultante ser√° salvo. Nenhuma sa√≠da expl√≠cita √© retornada.\"\"\"\n",
        "    video = AudioFileClip(mp3_file)\n",
        "    video.write_audiofile(wav_file)\n",
        "\n",
        "  # Criando a fun√ß√£o que gera a transcri√ß√£o:\n",
        "@st.cache_data\n",
        "def get_transcriptions(url):\n",
        "    \"\"\" Fun√ß√£o que recebe um link de um v√≠deo no YouTube e\n",
        "    devolve um dicion√°rio contendo o t√≠tulo do v√≠deo (key)\n",
        "    e a transcri√ß√£o de seu √°udio (value). \"\"\"\n",
        "    dicionario = {}\n",
        "      # Baixando o √°udio:\n",
        "    youtube_content = YouTube(url)\n",
        "    title = youtube_content.title\n",
        "    title = re.sub('[^A-z0-9 -]', '', title).replace(\" \", \" \")\n",
        "    audio_streams = youtube_content.streams.filter(only_audio=True)\n",
        "    audio_streams[0].download(filename=f\"{title}.mp3\")\n",
        "      # Convertendo para Wav:\n",
        "    cwd = os.getcwd()\n",
        "    mp3_file = os.path.join(cwd, f\"{title}.mp3\")\n",
        "    wav_file = os.path.join(cwd, f\"{title}.wav\")\n",
        "    convert_mp3_to_wav(mp3_file, wav_file)\n",
        "      # Inicializando o reconhecedor de fala:\n",
        "    r = sr.Recognizer()\n",
        "      # Carregando o √°udio gravado pelo Whisper em um objeto de √°udio:\n",
        "    with sr.AudioFile(wav_file) as source:\n",
        "        audio = r.record(source)\n",
        "      # Transcrevendo:\n",
        "    texto_transcrito = r.recognize_whisper(audio)\n",
        "      # Adicionando ao dicionario:\n",
        "    dicionario[title] = texto_transcrito\n",
        "    return dicionario\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o que carrega o sumarizador extrativo (para portugu√™s e ingl√™s) via HuggingFace:\n",
        "@st.cache_resource\n",
        "def load_extractive():\n",
        "    \"\"\" Fun√ß√£o sem valores de entrada que carrega o modelo\n",
        "     de sumariza√ß√£o extrativa via HuggingFace. \"\"\"\n",
        "    return pipeline(\"summarization\", model=\"NotXia/longformer-bio-ext-summ\", tokenizer=AutoTokenizer.from_pretrained(\"NotXia/longformer-bio-ext-summ\"), trust_remote_code=True)\n",
        "\n",
        "  # Criando a fun√ß√£o que gera a sumariza√ß√£o extrativa:\n",
        "@st.cache_data\n",
        "def get_summarization(_model_pipeline, full_text, ratio):\n",
        "    \"\"\" Fun√ß√£o que recebe um texto completo a ser resumido, juntamente\n",
        "    √† taxa desejada de sumariza√ß√£o e a pipeline do modelo que far√° o\n",
        "    sum√°rio, e devolve a vers√£o resumida desse texto. \"\"\"\n",
        "    sentences = nltk.sent_tokenize(full_text)\n",
        "    extractive_sentences = _model_pipeline({\"sentences\": sentences}, strategy=\"ratio\", strategy_args=ratio)\n",
        "    extractive_text = \" \".join(extractive_sentences[0])\n",
        "    return extractive_text\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o que gera a tradu√ß√£o do texto a partir do ChatGPT:\n",
        "@st.cache_data\n",
        "def get_translation(summarized_text):\n",
        "    \"\"\" Fun√ß√£o que recebe um resumo a ser traduzido,\n",
        "    juntamente √† pipeline do modelo que far√° a tradu√ß√£o,\n",
        "    e devolve a vers√£o traduzida do texto. \"\"\"\n",
        "    LLM = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.25, model_name=\"gpt-3.5-turbo\")\n",
        "    messages = [SystemMessage(content='Voc√™ √© um especialista em tradu√ß√µes de texto do ingl√™s para portugu√™s'), HumanMessage(content=summarized_text)]\n",
        "    translation = LLM(messages)\n",
        "    return translation.content\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o que corrige os erros de transcri√ß√£o do texto a partir do ChatGPT:\n",
        "@st.cache_data\n",
        "def get_correction(transcription):\n",
        "    \"\"\" Fun√ß√£o que recebe a transcri√ß√£o (em PT-BR) e corrige eventuais\n",
        "    erros do whisper - por exemplo, a troca de palavras que, embora\n",
        "    semelhantes na pron√∫ncia, s√£o totalmente absurdas no contexto. \"\"\"\n",
        "    correction_prompt = PromptTemplate.from_template(template = 'Voc√™ √© um especialista em corre√ß√£o de textos. Voc√™ receber√° uma transcri√ß√£o de um √°udio e dever√° substituir as palavras transcritas erroneamente, considerando apenas a pron√∫ncia, por palavras adequadas que fa√ßam sentido no contexto do texto. Al√©m disso, corrija quest√µes gram√°ticais para facilitar a compreens√£o. Corrija a seguinte transcri√ß√£o: {transcription}')\n",
        "    LLM = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1, model_name=\"gpt-3.5-turbo\")\n",
        "    corrector = LLMChain(prompt=correction_prompt, llm=LLM)\n",
        "    with get_openai_callback() as cb:\n",
        "        corrected_transcription = corrector.run({\"transcription\": transcription})\n",
        "        total_t = cb.total_tokens\n",
        "        prompt_t = cb.prompt_tokens\n",
        "        completion_t = cb.completion_tokens\n",
        "        total_cost = cb.total_cost\n",
        "    return corrected_transcription\n",
        "\n",
        "\n",
        "  # Criando o chatbot\n",
        "def alan_videos(vectorstore):\n",
        "    \"\"\" Fun√ß√£o que inicializa e configura um LLM da OpenAI\n",
        "    e retorna um chatbot configurado pronto para uso. \"\"\"\n",
        "    memory = ConversationBufferWindowMemory(memory_key='chat_history', return_messages=True, k=3)\n",
        "    LLM = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.25, model_name=\"gpt-3.5-turbo\")\n",
        "    retriever=vectorstore.as_retriever()\n",
        "    chatbot = ConversationalRetrievalChain.from_llm(llm=LLM, retriever=retriever, memory=memory)\n",
        "    return chatbot\n",
        "\n",
        "  # Criando um modelo de chat:\n",
        "def chat(pergunta):\n",
        "    \"\"\" Fun√ß√£o que processa uma pergunta utilizando o chatbot\n",
        "    configurado (alan_videos) e retorna sua resposta. \"\"\"\n",
        "    with get_openai_callback() as cb:\n",
        "        resposta = st.session_state.alanvideos.invoke({\"question\": pergunta})\n",
        "        total_t = cb.total_tokens\n",
        "        prompt_t = cb.prompt_tokens\n",
        "        completion_t = cb.completion_tokens\n",
        "        total_cost = cb.total_cost\n",
        "    return resposta['answer']\n",
        "\n",
        "\n",
        "  # Criando a fun√ß√£o para customiza√ß√£o com CSS:\n",
        "def local_css(file_name):\n",
        "    \"\"\" Fun√ß√£o que carrega um arquivo CSS local e aplica o estilo\n",
        "    ao Streamlit app, personalizando a interface do usu√°rio. \"\"\"\n",
        "    with open(file_name, \"r\") as f:\n",
        "        st.markdown(f\"<style>{f.read()}</style>\", unsafe_allow_html=True)\n",
        "\n",
        "  # Configurando a aba do site:\n",
        "icon = Image.open(\"Tra√ßado laranja #f1863d.png\")\n",
        "st.set_page_config(page_title=\"AlanVideos\", page_icon=icon, layout=\"wide\", initial_sidebar_state=\"auto\")\n",
        "\n",
        "\n",
        "  # Configurando o site:\n",
        "def main():\n",
        "    local_css(\"style.css\")\n",
        "\n",
        "    header = st.container()\n",
        "    model = st.container()\n",
        "    model_1 = st.container()\n",
        "    model_2 = st.container()\n",
        "\n",
        "      # Configurando a barra lateral:\n",
        "    with st.sidebar:\n",
        "        with st.form(\"data_collection\"):\n",
        "            link = st.text_area(label=\"Coloque o link do seu v√≠deo do YouTube:\", height=25, placeholder=\"Digite seu link...\")\n",
        "            language = st.selectbox('Qual a linguagem do seu v√≠deo?', ('Portugu√™s (pt)', 'Ingl√™s (en)'))\n",
        "            translate = st.selectbox('Voc√™ deseja que o seu v√≠deo seja traduzido para Portugu√™s?', ('N√£o', 'Sim'))\n",
        "            compression_rate = st.slider(label=\"Selecione o percentual de sumariza√ß√£o:\", min_value=0.05, max_value=0.50, value=0.25, step=0.05)\n",
        "            submitted = st.form_submit_button(\"Enviar\")\n",
        "\n",
        "            # Verificando se o link do v√≠deo √© v√°lido:\n",
        "            if link != '':\n",
        "                if re.match((r'(https?://)?(www\\.)?''(youtube|youtu|youtube-nocookie)\\.(com|be)/''(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'), link):\n",
        "                    st.success('Dados coletados!', icon=\"‚úÖ\")\n",
        "                else:\n",
        "                    st.error('Link inv√°lido. Por favor, insira um link do YouTube.', icon=\"üö®\")\n",
        "\n",
        "    if \"alanvideos\" not in st.session_state:\n",
        "        st.session_state.alanvideos = None\n",
        "\n",
        "      # Configurando o cabe√ßalho:\n",
        "    with header:\n",
        "        st.title(\":orange[Alan]Videos\")\n",
        "        st.subheader(\"Ol√°, usu√°rio! Este √© um projeto que utiliza t√©cnicas de intelig√™ncia artificial para simplificar e acelerar a compreens√£o de conte√∫do audiovisual. Por favor, preencha o formul√°rio ao lado para que possamos responder as suas d√∫vidas a respeito de um v√≠deo do YouTube!  :)\", divider = \"orange\")\n",
        "\n",
        "      # Configurando os modelos:\n",
        "    with model:\n",
        "        if submitted and re.match((r'(https?://)?(www\\.)?''(youtube|youtu|youtube-nocookie)\\.(com|be)/''(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'), link):\n",
        "            with st.spinner(\"Carregando modelos...\"):\n",
        "                nltk.download(\"punkt\")\n",
        "                extractive = load_extractive()\n",
        "            with st.spinner(\"Transcrevendo texto...\"):\n",
        "                transcription = get_transcriptions(link)\n",
        "                texto_cru = ''.join(list(transcription.values()))\n",
        "              # Preparando o modelo de sumariza√ß√£o ap√≥s o envio de um link:\n",
        "            with model_1:\n",
        "                st.header(\"Texto Sumarizado:\")\n",
        "                with st.spinner(\"Carregando sumariza√ß√£o...\"):\n",
        "                    summary = get_summarization(extractive, texto_cru, compression_rate)\n",
        "                      # Usando o GPT para corrigir a transcri√ß√£o, caso o v√≠deo seja em PT-BR:\n",
        "                    if language == 'Portugu√™s (pt)':\n",
        "                        summary = get_correction(summary)\n",
        "                      # Usando o GPT para traduzir a transcri√ß√£o para PT-BR, caso o usu√°rio prefira:\n",
        "                    elif language == 'Ingl√™s (en)' and translate == 'Sim':\n",
        "                        with st.spinner(\"Traduzindo sumariza√ß√£o...\"):\n",
        "                            summary = get_translation(summary)\n",
        "                    st.session_state['summary'] = summary\n",
        "              # Preparando o AlanVideos ap√≥s o envio de um link:\n",
        "            with model_2:\n",
        "                st.header(\"Resposta das perguntas:\")\n",
        "                with st.spinner(\"Preparando AlanVideos...\"):\n",
        "                      # Separando o texto em chunks:\n",
        "                    chunks = get_chunks(texto_cru)\n",
        "                      # Armazenando os peda√ßos de texto em embeddings:\n",
        "                    vectorstore = get_vectorstore(chunks)\n",
        "                      # Criando o chatbot:\n",
        "                    st.session_state.alanvideos = alan_videos(vectorstore)\n",
        "\n",
        "      # Apresentando os resultados:\n",
        "    with model:\n",
        "          # Resultado do modelo de sumariza√ß√£o:\n",
        "        with model_1:\n",
        "            if 'summary' in st.session_state:\n",
        "                st.write(st.session_state['summary'])\n",
        "      # Resultado do AlanVideos:\n",
        "    if 'summary' in st.session_state:\n",
        "        if \"chat_history\" not in st.session_state:\n",
        "            st.session_state.chat_history = [AIMessage(content=f\"Ol√°, meu nome √© Alan, e estou aqui para responder perguntas sobre o v√≠deo. Como posso ajudar?\")]\n",
        "        pergunta = st.chat_input('Fa√ßa uma pergunta sobre o v√≠deo: ')\n",
        "        if pergunta is not None and pergunta != \"\":\n",
        "            resposta = chat(pergunta)\n",
        "            st.session_state.chat_history.append(HumanMessage(content=pergunta))\n",
        "            st.session_state.chat_history.append(AIMessage(content=resposta))\n",
        "        for message in st.session_state.chat_history:\n",
        "            if isinstance(message, AIMessage):\n",
        "                with st.chat_message(\"AI\"):\n",
        "                    st.write(message.content)\n",
        "            elif isinstance(message, HumanMessage):\n",
        "                with st.chat_message(\"Human\"):\n",
        "                    st.write(message.content)\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPfm2EHyimcF"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmF1j2csipF8"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}